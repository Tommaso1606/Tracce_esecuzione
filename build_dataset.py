import pandas as pd
import numpy as np
import configparser as cp
import sys

from drain3 import TemplateMiner
from drain3.template_miner_config import TemplateMinerConfig
from drain3.file_persistence import FilePersistence


# Read the configuration file
# Example of configuration file
# [Config]
#
# TRACE_PATH = /path/to/csv/file
# RUN_LOG_PATH = /path/to/csv/file
# BUSINESS_LOG_PATH = /path/to/csv/file
# FOLDER_LOG_BY_TRACE_PATH = /path/to/folder
# DATASET_FILE_PATH = /path/to/folder
# DRAIN_CONFIG_FILE_PATH = /path/to/ini/file
# DRAIN_LOG_PRESISTENCE_FILE_PATH = /path/to/bin/file (i.e., the bin file generated by Drain after learning)
# DRAIN_ENDPOINT_PRESISTENCE_FILE_PATH = /path/to/bin/file (i.e., the bin file generated by Drain after learning)

# ENABLE_WRITE_LOGS_ON_FILES = Boolean value (i.e., True or False) indicating if logs collected for each trace_id should be written also on files for analysis purposes
# GENERATE_JSON = Boolean value (i.e., True or False) indicating if the final dataset has to be written in a JSON file
# JSON_INDENT = Integer value indicating the level of indentation to use in the JSON file (use 0 for no indentation, i.e., the whole JSON on a single row, while use 1 to have a multi-row JSON)
# RAW_LOGS_AS_STRING = Boolean value (i.e., True or False) indicating if the raw logs have to be written as a string, instead of an array
# LOG_TEMPLATES_AS_STRING = Boolean value (i.e., True or False) indicating if the log templates have to be written as a string, instead of an array
# INVOLVED_SERVICES_AS_STRING = Boolean value (i.e., True or False) indicating if the involved services have to be written as a string, instead of an array
# INVOLVED_URLS_AS_STRING = Boolean value (i.e., True or False) indicating if the involved urls have to be written as a string, instead of an array
def read_configuration(config_file_path):

    global TRACE_PATH
    global RUN_LOG_PATH
    global RUN_LOG_PATH
    global BUSINESS_LOG_PATH
    global FOLDER_LOG_BY_TRACE_PATH
    global DATASET_FILE_PATH
    global ENABLE_WRITE_LOGS_ON_FILES
    global GENERATE_JSON
    global JSON_INDENT
    global DRAIN_CONFIG_FILE_PATH
    global DRAIN_LOG_PRESISTENCE_FILE_PATH
    global DRAIN_ENDPOINT_PRESISTENCE_FILE_PATH
    global RAW_LOGS_AS_STRING
    global LOG_TEMPLATES_AS_STRING
    global INVOLVED_SERVICES_AS_STRING
    global INVOLVED_URLS_AS_STRING

    configParser = cp.RawConfigParser()  
    configParser.read(config_file_path)

    TRACE_PATH = configParser.get('Config', 'TRACE_PATH')
    RUN_LOG_PATH = configParser.get('Config', 'RUN_LOG_PATH')
    BUSINESS_LOG_PATH = configParser.get('Config', 'BUSINESS_LOG_PATH')
    FOLDER_LOG_BY_TRACE_PATH = configParser.get('Config', 'FOLDER_LOG_BY_TRACE_PATH')
    DATASET_FILE_PATH = configParser.get('Config', 'DATASET_FILE_PATH')
    ENABLE_WRITE_LOGS_ON_FILES = configParser.get('Config', 'ENABLE_WRITE_LOGS_ON_FILES')
    GENERATE_JSON = eval(configParser.get('Config', 'GENERATE_JSON'))
    JSON_INDENT = eval(configParser.get('Config', 'JSON_INDENT'))
    DRAIN_CONFIG_FILE_PATH = configParser.get('Config', 'DRAIN_CONFIG_FILE_PATH')
    DRAIN_LOG_PRESISTENCE_FILE_PATH = configParser.get('Config', 'DRAIN_LOG_PRESISTENCE_FILE_PATH')
    DRAIN_ENDPOINT_PRESISTENCE_FILE_PATH = configParser.get('Config', 'DRAIN_ENDPOINT_PRESISTENCE_FILE_PATH')
    RAW_LOGS_AS_STRING = eval(configParser.get('Config', 'RAW_LOGS_AS_STRING'))
    LOG_TEMPLATES_AS_STRING = eval(configParser.get('Config', 'LOG_TEMPLATES_AS_STRING'))
    INVOLVED_SERVICES_AS_STRING = eval(configParser.get('Config', 'INVOLVED_SERVICES_AS_STRING'))
    INVOLVED_URLS_AS_STRING = eval(configParser.get('Config', 'INVOLVED_URLS_AS_STRING'))


# Generate the json file containing the dataset
def generate_JSON(data_frame):

    print('Generating the JSON file with the dataset ... ')
    #data_frame.to_json(DATASET_FILE_PATH + '/dataset.json', orient ='records', indent=JSON_INDENT) # Leads to out-of-memory for large datasets
    with open(DATASET_FILE_PATH + '/dataset.json', 'w') as f:
        index = 0
        if (JSON_INDENT == 0):
            f.write("[")    
        else:
            f.write("[\n")
            
        for i in data_frame.index:

            f.write(data_frame.loc[i].to_json(indent=JSON_INDENT))

            index = index + 1
            if (JSON_INDENT == 0):
                if (index != len(data_frame)):
                    f.write(",\n")
            else:
                if index == len(data_frame):
                    f.write("\n")
                else:
                    f.write(",\n")

        f.write("]")
    print('     completed!\n')


# Prepare drain to get the log templates
def prepare_drain(persistence_file_path):

    # Read the configuration file
    config = TemplateMinerConfig()
    config.load(DRAIN_CONFIG_FILE_PATH)
    config.profiling_enabled = True


    # Set the persistance file
    persistence = FilePersistence(persistence_file_path)


    # Get the miner
    template_miner = TemplateMiner(persistence, config=config)

    return template_miner

# Get the templates and parameters of log or endpoints using the trained drain
def get_templates(entries, template_miner):

    templates = []
    parameters = []
    template_matching = [0] * len(template_miner.drain.clusters)

    # Analyze each entry
    for entry in entries :

        # Check for templates
        template = template_miner.match(entry)

        # Check if a template has been found
        if template is not None:

            # Append the template to the list of templates
            templates.append(template.get_template())

            # Increment the count of template matching
            template_matching[template.cluster_id-1] += 1

            # Append the parameters to the list of parameteres for the template
            parameters.append(template_miner.get_parameter_list(template.get_template(), entry))

        else:
            print("no-match-found")

    return templates, parameters, template_matching




def main():

    # Check the presence of the configuration file as argument
    if len(sys.argv) != 2:

        print ("The configuration file path is missing!")
        exit(0)

    # Get the configuration file path
    CONFIG_FILE_PATH = sys.argv[1]

    # Read configuration file
    read_configuration(CONFIG_FILE_PATH)


    # Configure drain for log templates and endpoint templates
    log_template_miner = prepare_drain(DRAIN_LOG_PRESISTENCE_FILE_PATH)
    endpoint_template_miner = prepare_drain(DRAIN_ENDPOINT_PRESISTENCE_FILE_PATH)


    # Read traces
    print('Reading the traces ... ')
    df_trace = pd.read_csv(TRACE_PATH)
    print('     completed!\n')

    # Convert the timestamp
    print('Converting timestamps of traces ... ')
    #df_trace['timestamp'] = pd.to_datetime(df_trace['timestamp'], errors = 'coerce')
    start_time_ms = pd.to_datetime(df_trace['start_time'], errors = 'coerce', format='%Y-%m-%d %H:%M:%S.%f')
    start_time_no_ms = pd.to_datetime(df_trace['start_time'], errors = 'coerce', format='%Y-%m-%d %H:%M:%S')
    start_time_ms.fillna(start_time_no_ms)
    df_trace['start_time'] = start_time_ms

    end_time_ms = pd.to_datetime(df_trace['end_time'], errors = 'coerce', format='%Y-%m-%d %H:%M:%S.%f')
    end_time_no_ms = pd.to_datetime(df_trace['end_time'], errors = 'coerce', format='%Y-%m-%d %H:%M:%S')
    end_time_ms.fillna(end_time_no_ms)
    df_trace['end_time'] = end_time_ms
    print('     completed!\n')


    # Sort by trace_id and strart_time
    print('Sorting the traces ... ')
    df_trace.sort_values(by=['trace_id', 'start_time'], inplace=True)
    print('     completed!\n')


    # Read the run logs
    print('Reading the run logs ... ')
    df_rlog = pd.read_csv(RUN_LOG_PATH, delimiter=";")
    print('     completed!\n')

    # Convert the timestamp
    print('Converting timestamp of run logs ... ')
    timestamp_ms = pd.to_datetime(df_rlog['timestamp'], errors = 'coerce', format='%Y-%m-%d %H:%M:%S,%f')
    timestamp_no_ms = pd.to_datetime(df_rlog['timestamp'], errors = 'coerce', format='%Y-%m-%d %H:%M:%S')
    timestamp_ms.fillna(timestamp_no_ms)
    df_rlog['timestamp'] = timestamp_ms
    print('     completed!\n')

    # Set the timestamp as index and order the index
    print('Sorting the run logs ... ')
    df_rlog = df_rlog.set_index('timestamp')
    df_rlog = df_rlog.sort_index()
    print('     completed!\n')


    # Read the business logs
    print('Reading the business logs ... ')
    df_blog = pd.read_csv(BUSINESS_LOG_PATH, delimiter=";")
    print('     completed!\n')

    # Convert the timestamp
    print('Converting timestamp of business logs ... ')
    timestamp_ms = pd.to_datetime(df_blog['timestamp'], errors = 'coerce', format='%Y-%m-%d %H:%M:%S,%f')
    timestamp_no_ms = pd.to_datetime(df_blog['timestamp'], errors = 'coerce', format='%Y-%m-%d %H:%M:%S')
    timestamp_ms.fillna(timestamp_no_ms)
    df_blog['timestamp'] = timestamp_ms
    print('     completed!\n')

    # Set the timestamp as index and order the index
    print('Sorting the business logs ... ')
    df_blog = df_blog.set_index('timestamp')
    df_blog = df_blog.sort_index()
    print('     completed!\n')


    # For each trace_id, get the min start time, max end time, the involved services, involved urls from traces and status code
    print('Getting the info from traces ... ')
    min_start_time_by_traceid = df_trace.groupby('trace_id')['start_time'].min()
    max_end_time_by_traceid = df_trace.groupby('trace_id')['end_time'].max()
    #services_by_traceid = df_trace.groupby('trace_id')['service_name'].unique()
    services_by_traceid = df_trace.groupby('trace_id')['service_name']
    unique_services_by_traceid = services_by_traceid.unique()
    #urls_by_traceid = df_trace.groupby('trace_id')['url'].unique()
    urls_by_traceid = df_trace.groupby('trace_id')['url']
    #status_codes_by_traceid = df_trace.groupby('trace_id')['status_code'].unique()
    status_codes_by_traceid = df_trace.groupby('trace_id')['status_code']
    status_codes_by_traceid_unique = df_trace['status_code'].unique()
    print('     completed!\n')


    # Get the different unique severity levels from logs (this is dependent on the log format)
    print('Getting the severity levels from logs ... ')
    tot_logs = pd.concat([df_rlog, df_blog], axis=0)
    unique_severity_levels = tot_logs['log'].str.split('|').apply(lambda x : x[1].replace(' ','')).unique()
    print('     completed!\n')


    # Get the log template IDs from drain
    log_clusters = log_template_miner.drain.clusters


    # Prepare the header of the dataset
    header = ["trace_id",
            "first_service",
            "first_url",
            "involved_services",
            "involved_urls",
            "services_number",
            "unique_services_number",
            "min_start_time",
            "max_end_time",
            "latency_sec",
            "r_log_entries",
            "b_log_entries",
            "tot_log_entries",
            "raw_logs",
            "logs_templates",
            ]

    # Append log_templates to the header
    if (LOG_TEMPLATES_AS_STRING == True):
        header.append("logs_templates_parameters")

    # Append status codes to the header
    for status_code in status_codes_by_traceid_unique:
        header.append(status_code)

    # Append log severity levels to the header
    for severity_level in unique_severity_levels:
        header.append(severity_level)

    # Append the log template IDs to the header
    for log_cluster in log_clusters:
        header.append('event_' + str(log_cluster.cluster_id))


    # Get the uniqe trace_id
    trace_ids = df_trace['trace_id'].unique()


    # List for storing entries to write in the dataset
    data = []


    # Iterate on the unique trace_ids
    print('Analyzing logs by trace_id ...')
    for trace_id in trace_ids:

        # Create a string with all involved services, or list
        if (INVOLVED_SERVICES_AS_STRING == True):
            #services = '--'.join(services_by_traceid[trace_id].astype(str)) #used with unique() applied on services_by_traceid
            services = '--'.join(services_by_traceid.get_group(trace_id).astype(str))
        else:
            services = str(services_by_traceid.get_group(trace_id).tolist())

        # Create a string with all involved endpoints, or list
        if (INVOLVED_URLS_AS_STRING == True):
            #urls = '--'.join(urls_by_traceid[trace_id].astype(str)) #used with unique() applied on urls_by_traceid
            urls = '--'.join(urls_by_traceid.get_group(trace_id).astype(str))
        else:
            urls = str(urls_by_traceid.get_group(trace_id).tolist())


        # Get the run logs that match the trace time range, i.e., min_start_time - max_end_time
        rlogs = df_rlog.loc[str(min_start_time_by_traceid[trace_id]):str(max_end_time_by_traceid[trace_id])]

        # Get the run logs containing the trace id (only for logs where the trace id is indicated)
        rlogs = rlogs[rlogs['log'].str.contains(trace_id)]


        # Get the business logs that match the trace time range, i.e., min_start_time - max_end_time
        blogs = df_blog.loc[str(min_start_time_by_traceid[trace_id]):str(max_end_time_by_traceid[trace_id])]
        
        # Get the run logs containing the trace id (only for logs where the trace id is indicated)
        blogs = blogs[blogs['log'].str.contains(trace_id)]


        # Concat run and bussiness logs
        tot_logs = pd.concat([rlogs, blogs], axis=0)

        
        if tot_logs['log'].size > 0 :

            # Sort the log by timestamp
            tot_logs.sort_values(by=['timestamp'], inplace=True)

            # Write the logs to a file named with the trace_id, if needed
            if (ENABLE_WRITE_LOGS_ON_FILES == True):
            
                with open(FOLDER_LOG_BY_TRACE_PATH + '/' + trace_id + '.txt', 'w') as f:
                    tot_logs['log'].values.tofile(f, sep='\n', format='%s')

            # Create a string containing all the logs entries to include in the dataset, or list
            if (RAW_LOGS_AS_STRING == True):
                raw_logs = '\\n'.join(tot_logs['log'].astype(str))
            else:
                raw_logs = tot_logs['log'].tolist()


            # Get templates and parameters for logs

            # Ask drain for template and parameters    
            logs_templates, log_parameters, logs_template_matching = get_templates(tot_logs['log'], log_template_miner)

            # Create a string containing all the logs templates to include in the dataset, or list
            if (LOG_TEMPLATES_AS_STRING == True):
                logs_templates_string = '\\n'.join(logs_templates)
                logs_templates_parameters_string = '\\n'.join(map(str,log_parameters))
            else:
                logs_templates_and_parameters = []
                for i in range(len(logs_templates)):
                    temp_dict = {'template' : logs_templates[i], 'parameters' : log_parameters[i]}
                    logs_templates_and_parameters.append(temp_dict)

            # Get the latency
            latency = (max_end_time_by_traceid[trace_id]-min_start_time_by_traceid[trace_id]).total_seconds()


            # Prepare the entry of the dataset
            entry_to_write = [str(trace_id), 
                            #str(services_by_traceid[trace_id][0]), #used with unique() applied on services_by_traceid
                            str(services_by_traceid.get_group(trace_id).iloc[0]),
                            #str(urls_by_traceid[trace_id][0]), #used with unique() applied on urls_by_traceid
                            str(urls_by_traceid.get_group(trace_id).iloc[0]), 
                            str(services),
                            str(urls),
                            #str(len(services_by_traceid[trace_id])), #used with unique() applied on services_by_traceid
                            len(services_by_traceid.get_group(trace_id)),
                            len(unique_services_by_traceid[trace_id]),
                            str(min_start_time_by_traceid[trace_id]),
                            str(max_end_time_by_traceid[trace_id]),
                            latency,
                            rlogs.size,
                            blogs.size,
                            tot_logs.size,
                            raw_logs,
                            ]
            
            # Append the log templates as string or list
            if (LOG_TEMPLATES_AS_STRING == True):
                entry_to_write.append(str(logs_templates_string))
                entry_to_write.append(str(logs_templates_parameters_string).replace('"', "'"))
            else:
                entry_to_write.append(logs_templates_and_parameters)
            
            # Append the status codes
            #unique, counts = np.unique(status_codes_by_traceid[trace_id], return_counts=True) #used with unique() applied on status_codes_by_traceid
            unique, counts = np.unique(status_codes_by_traceid.get_group(trace_id), return_counts=True)
            status_code_dict = dict(zip(unique, counts))
        
            for status_code in status_codes_by_traceid_unique:
                if status_code in status_code_dict:
                    entry_to_write.append(status_code_dict[status_code])
                else:
                    entry_to_write.append(0)


            # Append the log severity amounts
            for severity_level in unique_severity_levels:
                entry_to_write.append(len(np.flatnonzero(np.core.defchararray.find(tot_logs['log'].values.astype(str),severity_level)!=-1)))


            # Append the amount of matching for each template
            entry_to_write.extend(logs_template_matching)
            

            # Append the entry to the list containg data of the dataset
            data.append(entry_to_write)

    print('     completed!\n')


    # Generate the dataframe containing the dataset
    dataset_df = pd.DataFrame(data, columns=header)


    # Generate the json file containing the dataset, if needed
    if GENERATE_JSON == True:
        generate_JSON(dataset_df)


if __name__ == "__main__":
    main()